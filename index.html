<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Privacy Preserving Machine Learning (NeurIPS 2018 Workshop)</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <span class="light">PPML'18</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">Scope</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">CFP &amp; Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#speakers">Invited Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#schedule">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#grants">Travel Grants</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#organizers">Organizers</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">Privacy Preserving Machine Learning</h1>
                        <p class="intro-text">NeurIPS 2018 Workshop
                            <br />Montréal, December 8
                        </p>
                        <p class="location-text">
                            Palais des Congrès de Montréal
                            <br /> Room: 512CDGH
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Scope</h2>
                <p>This one day workshop focuses on privacy preserving techniques for training, inference, and disclosure in large scale data analysis, both in the distributed and centralized settings. We have observed increasing interest of the ML community in leveraging cryptographic techniques such as Multi-Party Computation (MPC) and Homomorphic Encryption (HE) for privacy preserving training and inference, as well as Differential Privacy (DP) for disclosure. Simultaneously, the systems security and cryptography community has proposed various secure frameworks for ML. We encourage both theory and application-oriented submissions exploring a range of approaches, including:</p>
                <ul class="list-group">
                    <li class="list-group-item speaker">secure multi-party computation techniques for ML</li>
                    <li class="list-group-item speaker">homomorphic encryption techniques for ML</li>
                    <li class="list-group-item speaker">hardware-based approaches to privacy preserving ML</li>
                    <li class="list-group-item speaker">centralized and decentralized protocols for learning on encrypted data</li>
                    <li class="list-group-item speaker">differential privacy: theory, applications, and implementations</li>
                    <li class="list-group-item speaker">statistical notions of privacy including relaxations of differential privacy</li>
                    <li class="list-group-item speaker">empirical and theoretical comparisons between different notions of privacy</li>
                    <li class="list-group-item speaker">trade-offs between privacy and utility</li>
                </ul>
                <p>We think it will be very valuable to have a forum to unify different perspectives and start a discussion about the relative merits of each approach. The workshop will also serve as a venue for networking people from different communities interested in this problem, and hopefully foster fruitful long-term collaboration.</p>
            </div>
        </div>
    </section>

    <!-- CFP & Dates Section -->
        <section id="dates" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Call For Papers &amp; Important Dates</h2>
                    <a href="cfp-ppml18.txt" class="btn btn-default btn-lg">Download Full CFP</a>
                    <br/>
                    <br/>
                    <br/>
                    <p><b>Submission deadline</b>: <s>October 8</s> October 16, 2018 (11:59pm AoE)
                        <br/><b>Notification of acceptance</b>: November 1, 2018
                        <!-- <br/><b>NIPS early <a href="https://nips.cc/Register/view-registration">registration</a> deadline</b>: October 24, 2018 -->
                        <br/><b>Workshop</b>: December 8, 2018</p>
                    <h3>Submission Instructions</h3>
                    <p>
                        Submissions in the form of extended abstracts must be <b>at most 4 pages long</b> (not including references and an unlimited number of pages for supplemental material, which reviewers are not required to take into account) and <b>adhere to the <a href="https://nips.cc/Conferences/2018/PaperInformation/StyleFiles">NeurIPS format</a></b>. We do accept submissions of work recently published or currently under review. Submissions should be anonymized. The workshop will not have formal proceedings, but authors of accepted abstracts can choose to have a link to arxiv or a pdf published on the workshop webpage.
                    </p>
                    <p>
                        If the new notification date causes issues with a potential visa application that depends specifically on the acceptance at this workshop, please contact us directly at <a href="mailto:ppml18@easychair.org?Subject=Regarding%20visa%20application" target="_top">ppml18@easychair.org</a>.
                    </p>
                    <p style="color: #d07200;">
                        We can offer the opportunity to purchase a NeurIPS registration to <b>one</b> author of each accepted paper.
                    </p>
                    <p>
                        From the <b><a href="https://nips.cc/Conferences/2018/AcceptedWorkshopFAQ">Workshop FAQ</a></b>:
                        <i>the reserve tickets guarantee attendance to the workshops, and depending on availability, also to the main conference and tutorials. We expect most of the reserve tickets to allow registration for tutorials, conference and workshops, but again, only the workshops part is for certain.</i>
                    </p>
                </div>
            </div>
        </section>

    <!-- Speakers Section -->
    <section id="speakers" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Invited Speakers</h2>
                <ul class="list-group">
                    <li class="list-group-item speaker"><a href="https://cseweb.ucsd.edu/~kamalika/">Kamalika Chaudhuri</a> (University of California, San Diego )</li>
                    <li class="list-group-item speaker"><a href="https://people.csail.mit.edu/shafi/">Shafi Goldwasser</a> (MIT & Weizmann Institute of Science)</li>
                    <li class="list-group-item speaker"><a href="http://www.iangoodfellow.com/">Ian Goodfellow</a> (Google Brain)</li>
                    <li class="list-group-item speaker"><a href="http://www.cse.psu.edu/~ads22/">Adam Smith</a> (Boston University)</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Schedule Section -->
    <section id="schedule" class="container content-section text-center">
        <div class="row">
            <div class="col-sm-8 col-sm-offset-2">
                <h2>Schedule</h2>
                <table class="table schedule">
                    <tbody>
                    <tr>
                        <td class="time">8:30</td>
                        <td class="slot">Welcome and Introduction</td>
                    </tr>

                    <tr>
                        <td class="time">8:50</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs2" data-toggle="collapse" class="accordion-toggle">
                        Ian Goodfellow
                        &mdash;
                        Scalable PATE and the Secret Sharer
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs2">
                            Coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:40</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs3" data-toggle="collapse" class="accordion-toggle">
                        Shafi Goldwasser
                        &mdash;
                        Machine Learning and Cryptography: Challenges and Opportunities
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs3">
                            At the mid eighties researchers in computational learning theory pointed the way to  examples of hard learning tasks such as learning parity with noise (LPN) and learning with errors (LWE) which have been extremely useful for building sophisticated cryptographic primitives such as homomorphic encryption which are unbreakable if LPN and LWE are hard to learn.
                            <br/>
                            Today, with the rise of machine learning algorithms that use large amounts of data to come up with procedures which have the potential to replace human decision processes, cryptography, in turn,  stands to provide machine learning, tools for keeping data private during both training and inference phases of ML and to provide methods to verify adherence of models with data. These promise to be important steps in ensuring the safe transition of  power from human to algorithmic decision making.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">10:30</td>
                        <td class="break">Coffee break</td>
                    </tr>

                    <tr>
                        <td class="time">11:00</td>
                        <td class="slot talk"><a href="#tabs4" data-toggle="collapse" class="accordion-toggle">
                        Privacy Amplification by Iteration
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Vitaly Feldman, Ilya Mironov, Kunal Talwar and Abhradeep Thakurta
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs4">
                            Many commonly used learning algorithms work by iteratively updating an intermediate solution using one or a few data points in each iteration. Analysis of differential privacy for such algorithms often involves ensuring privacy of each step and then reasoning about the cumulative privacy cost of the algorithm. This is enabled by composition theorems for differential privacy that allow releasing of all the intermediate results. In this work, we demonstrate that for contractive iterations, not releasing the intermediate results strongly amplifies the privacy guarantees.
                            <br/>
                            We describe several applications of this new analysis technique to solving convex optimization problems via noisy stochastic gradient descent. For example, we demonstrate that a relatively small number of non-private data points from the same distribution can be used to close the gap between private and non-private convex optimization. In addition, we demonstrate that we can achieve guarantees similar to those obtainable using the privacy-amplification-by-sampling technique in several natural settings where that technique cannot be applied.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:15</td>
                        <td class="slot talk"><a href="#tabs5" data-toggle="collapse" class="accordion-toggle">
                        Subsampled Renyi Differential Privacy and Analytical Moments Accountant
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Yu-Xiang Wang, Borja Balle and Shiva Kasiviswanathan
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs5">
                            We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms. Specifically, we provide a tight upper bound on the Renyi Differential Privacy (RDP) parameters for algorithms that: (1) subsample the dataset, and then (2) applies a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by Abadi et al. [CCS'16] for the Gaussian mechanism, to any subsampled RDP mechanism.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:30</td>
                        <td class="slot talk"><a href="#tabs6" data-toggle="collapse" class="accordion-toggle">
                        The Power of The Hybrid Model for Mean Estimation
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Yatharth Dubey and Aleksandra Korolova
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs6">
                            In this work we explore the power of the hybrid model of differential privacy (DP) proposed in~\cite{blender}, where some users desire the guarantees of the local model of DP and others are content with receiving the trusted curator model guarantees. In particular, we study the accuracy of mean estimation algorithms for arbitrary distributions in bounded support. We show that a hybrid mechanism which combines the sample mean estimates obtained from the two groups in an optimally weighted convex combination performs a constant factor better for a wide range of sample sizes than natural benchmarks. We analyze how this improvement factor is parameterized by the problem setting and how it varies with sample size.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:45</td>
                        <td class="slot talk"><a href="#tabs7" data-toggle="collapse" class="accordion-toggle">
                        Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar and Abhradeep Thakurta
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs7">
                            Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, when each user's value may change only a small number of times. We describe an algorithm whose privacy cost is polylogarithmic in the number of times the statistic is collected.
                            <br/>
                            More fundamentally---by building on anonymity of the users' reports---we also demonstrate how the privacy cost of our LDP algorithm can actually be much lower when viewed in the central model of differential privacy. We show, via a new and general technique, that any permutation-invariant algorithm satisfying $\varepsilon$-local differential privacy will satisfy $(O(\varepsilon \sqrt{\log \frac 1 \delta} / \sqrt{n}), \delta)$-central differential privacy. In the process, we clarify how the high noise and $\sqrt{n}$ overhead of LDP protocols is a consequence of them being significantly more private in the central model. As a final, practical corollary, our results also imply that industrial deployments of LDP mechanism may have much lower privacy cost than their advertised $\varepsilon$ would indicate---at least if reports are anonymized.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">12:00</td>
                        <td class="break">Lunch break</td>
                    </tr>

                    <tr>
                        <td class="time">13:30</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs8" data-toggle="collapse" class="accordion-toggle">
                        Kamalika Chaudhuri
                        &mdash;
                        Challenges in the Privacy-Preserving Analysis of Structured Data
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs8">
                            Much data analysis today is done on sensitive data, and particular privacy challenges arise when this data is sensitive and structured. In this talk I will describe two such challenges in the privacy-preserving analysis of complex, structured data that we have been working on in my group.
                            <br/>
                            The first is continual release of graph statistics in an online manner from an expanding graph, which is motivated by a problem in HIV epidemiology. Even though node differentially private release of graph statistics is highly challenging, here we will describe how we can get a differentially private solution for this problem that performs better than the natural sequential composition baseline.
                            <br/>
                            Next, I will talk about analysis of sensitive structured, correlated data, while still preserving the privacy of events in the data. Differential privacy does not adequately address privacy issues in this kind of data, and hence will look at a form of inferential privacy, called Pufferfish, that is more appropriate. We will provide mechanisms, establish their composition properties, and finally evaluate them on real data on physical activity measurements across time.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">14:20</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs9" data-toggle="collapse" class="accordion-toggle">
                        Adam Smith
                        &mdash;
                        Models for private data analysis of distributed data
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs9">
                            This talk will present a partial survey of the proposed (and implemented) models for private analysis of distributed data, together with some new results.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">15:10</td>
                        <td class="break">Coffee break</td>
                    </tr>

                    <tr>
                        <td class="time">11:45</td>
                        <td class="slot talk"><a href="#tabs10" data-toggle="collapse" class="accordion-toggle">
                        DP-MAC: The Differentially Private Method of Auxiliary Coordinates for Deep Learning
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Frederik Harder, Jonas Köhler, Max Welling and Mijung Park
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs10">
                            Developing a differentially private deep learning algorithm is challenging, due to the difficulty in analyzing the sensitivity of objective functions that are typically used to train deep neural networks. Many existing methods resort to the stochastic gradient descent algorithm and apply a pre-defined sensitivity to the gradients for privatizing weights. However, their slow convergence typically yields a high cumulative privacy loss. Here, we take a different route by employing the method of auxiliary coordinates, which allows us to independently update the weights per layer by optimizing a per-layer objective function. This objective function can be well approximated by a low-order Taylor's expansion, in which sensitivity analysis becomes tractable. We perturb the coefficients of the expansion for privacy, which we optimize using more advanced optimization routines than SGD for faster convergence. We empirically show that our algorithm provides a decent trained model quality under a modest privacy budget.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">15:45</td>
                        <td class="slot talk"><a href="#tabs11" data-toggle="collapse" class="accordion-toggle">
                        Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Florian Tramer and Dan Boneh
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs11">
                            As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted and untrusted devices. Building upon an efficient outsourcing scheme for matrix multiplication, we propose Slalom, a framework that securely delegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX or Sanctum) to a faster, yet untrusted, co-located processor. We evaluate Slalom by executing DNNs in an Intel SGX enclave, which selectively delegates work to an untrusted GPU. For two canonical DNNs, VGG16 and MobileNet, we obtain 20× and 6× increases in throughput for verifiable inference, and 11× and 4× for verifiable and private inference.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">16:00</td>
                        <td class="slot talk"><a href="#tabs12" data-toggle="collapse" class="accordion-toggle">
                        Secure Two Party Distribution Testing
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Alexandr Andoni, Tal Malkin and Negev Shekel Nosatzki
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs12">
                            We study the problem of discrete distribution testing in the two-party setting. For example, in the standard closeness testing problem, Alice and Bob each have t samples from, respectively, distributions a and b over [n], and they need to test whether a=b or a,b are ε-far (in the ℓ1 distance) for some fixed ε>0. This is in contrast to the well-studied one-party case, where the tester has unrestricted access to samples of both distributions, for which optimal bounds are known for a number of variations. Despite being a natural constraint in applications, the two-party setting has evaded attention so far.
                            <br/>
                            We address two fundamental aspects of the two-party setting: 1) what is the communication complexity, and 2) can it be accomplished securely, without Alice and Bob learning extra information about each other's input. Besides closeness testing, we also study the independence testing problem, where Alice and Bob have t samples from distributions a and b respectively, which may be correlated; the question is whether a,b are independent of ε-far from being independent.
                            <br/><br/>
                            Our contribution is three-fold:
                            <br/>
                            ∙ Communication: we show how to gain communication efficiency as we have more samples, beyond the information-theoretic bound on t. Furthermore, the gain is polynomially better than what one may obtain by adapting one-party algorithms.
                            <br/>
                            For the closeness testing, our protocol has communication s=Θ_ε(n^2/t^2) as long as t is at least the information-theoretic minimum number of samples. For the independence testing over domain [n]×[m], where n≥m, we obtain s=O_ε(n^2m/t^2+nm/t+m^1/2).
                            <br/>
                            ∙ Security: we define the concept of secure distribution testing and argue that it must leak at least some minimal information when the promise is not satisfied. We then provide secure versions of the above protocols with an overhead that is only polynomial in the security parameter.
                            <br/>
                            ∙ Lower bounds: we prove tightness of our trade-off for the closeness testing, as well as that the independence testing requires tight Ω(m^1/2) communication for unbounded number of samples. These lower bounds are of independent interest as, to the best of our knowledge, these are the first 2-party communication lower bounds for testing problems, where the inputs represent a set of i.i.d. samples.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">16:15</td>
                        <td class="slot talk"><a href="#tabs13" data-toggle="collapse" class="accordion-toggle">
                        Private Machine Learning in TensorFlow using Secure Computation
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Morten Dahl, Jason Mancuso, Yann Dupis, Ben DeCoste, Morgan Giraud, Ian Livingstone, Justin Patriquin and Gavin Uhma
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs13">
                            We present a framework for experimenting with secure multi-party computation directly in TensorFlow. By doing so we benefit from several properties valuable to both researchers and practitioners, including tight integration with ordinary machine learning processes, existing optimizations for distributed computation in TensorFlow, high-level abstractions for expressing complex algorithms and protocols, and an expanded set of familiar tooling. We give an open source implementation of a state-of-the-art protocol and report on concrete benchmarks using typical models from private machine learning.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">16:30</td>
                        <td class="slot talk"><a href="#tabs14" data-toggle="collapse" class="accordion-toggle">
                        Spotlight talks
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs14">
                            Order coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">17:15</td>
                        <td class="slot">Poster session</td>
                    </tr>

                    <tr>
                        <td class="time">18:15</td>
                        <td class="slot">Wrap up</td>
                    </tr>

        		    </tbody>
        		</table>
            </div>
        </div>
    </section>

    <!-- Accepted Papers -->
    <section id="papers" class="container content-section text-center">
        <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Accepted Papers</h2>
                <h4 style="color: #d07200;">
                Links to pdfs as well as abstracts will be added soon.
                </h4>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hsin-Pai Cheng, Patrick Yu, Haojing Hu, Feng Yan, Shiyu Li, Hai Li and Yiran Chen
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs1" class="paper-title">
                        LEASGD: an Efficient and Privacy-Preserving Decentralized Algorithm for Distributed Learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.11124" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs1" class="panel-footer panel-paper-footer collapse">
                    Distributed learning systems have enabled training large-scale models over a large amount of data in significantly shorter time. In this paper, we focus on decentralized distributed deep learning systems and aim to achieve differential privacy with good convergence rate and low communication cost. To achieve this goal, we propose a new learning algorithm LEASGD (Leader-Follower Elastic Averaging Stochastic Gradient Descent), which is driven by a novel Leader-Follower topology and a differential privacy model. We provide a theoretical analysis of the convergence rate and the trade-off between the performance and privacy in the private setting. The experimental results show that LEASGD outperforms state-of-the-art decentralized learning algorithm DPSGD by achieving steadily lower loss within the same iterations and by reducing the communication cost by 30%. In addition, LEASGD spends less differential privacy budget and has higher final accuracy result than DPSGD under private setting.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Garrett Bernstein and Daniel Sheldon
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs2" class="paper-title">
                            Differentially Private Bayesian Inference for Exponential Families
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1809.02188" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs2" class="panel-footer panel-paper-footer collapse">
                    The study of private inference has been sparked by growing concern regarding the analysis of data when it stems from sensitive sources. We present the first method for private Bayesian inference in exponential families that properly accounts for noise introduced by the privacy mechanism. It is efficient because it works only with sufficient statistics and not individual data. Unlike other methods, it gives properly calibrated posterior beliefs in the non-asymptotic data regime.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Cynthia Dwork and Vitaly Feldman
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs3" class="paper-title">
                            Privacy-preserving Prediction
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1803.10266" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs3" class="panel-footer panel-paper-footer collapse">
                    Ensuring differential privacy of models learned from sensitive user data is an important goal that has been studied extensively in recent years. It is now known that for some basic learning problems, especially those involving high-dimensional data, producing an accurate private model requires much more data than learning without privacy. At the same time, in many applications it is not necessary to expose the model itself. Instead users may be allowed to query the prediction model on their inputs only through an appropriate interface. Here we formulate the problem of ensuring privacy of individual predictions and investigate the overheads required to achieve it in several standard models of classification and regression.
                    <br/>
                    We first describe a simple baseline approach based on training several models on disjoint subsets of data and using standard private aggregation techniques to predict. We show that this approach has nearly optimal sample complexity for (realizable) PAC learning of any class of Boolean functions. At the same time, without strong assumptions on the data distribution, the aggregation step introduces a substantial overhead. We demonstrate that this overhead can be avoided for the well-studied class of thresholds on a line and for a number of standard settings of convex regression. The analysis of our algorithm for learning thresholds relies crucially on strong generalization guarantees that we establish for all differentially private prediction algorithms.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Bolin Ding, Janardhan Kulkarni and Sergey Yekhanin
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs4" class="paper-title">
                            A Distributed Algorithm For Differentially Private Heavy Hitters
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs4" class="panel-footer panel-paper-footer collapse">
                    In this paper we present a simple distributed algorithm for learning the heavy hitters in a stream of data in the local model of differential privacy. Our algorithm is efficient in the sense that its running time and communication complexity is polynomial in the input size. Furthermore, our algorithm achieves the optimal error guarantee for the frequency estimation.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Yunhui Long, Tanmay Gangwani, Haris Mughees and Carl Gunter
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs5" class="paper-title">
                            Distributed and Secure Machine Learning using Self-tallying Multi-party Aggregation
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.10296" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs5" class="panel-footer panel-paper-footer collapse">
                    Privacy preserving multi-party computation has many applications in areas like medicine and online advertisements. In this work, we focus on distributed and secure machine learning. In such a scenario, multiple parties are in possession of sensitive data which cannot be made public. At the same time, if different parties contribute their individual data to a shared pool and run a machine learning algorithm on the combined data, then much richer representative information can be gleaned as compared to using only the individual data pieces. In this work, we propose a framework for distributed secure machine learning among untrusted individuals. The framework consists of two parts: a 2-step training protocol based on homomorphic addition and a zero knowledge proof for data validity. By combining these two techniques, our framework provides privacy of per-user data, prevents against a malicious user contributing corrupted data to the shared pool, enables each user to self-compute the results of the algorithm without relying on external trusted third parties, and requires no private channels between groups of users. We show how different ML algorithms such as Latent Dirichlet Allocation, Na\"ive Bayes, Decision Trees etc. fit our framework for distributed, secure computing. We implement our protocol using Elliptic-Curve ElGamal, and show overheads for generating and verifying the zero-knowledge proofs which we use.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Di Wang, Adam Smith and Jinhui Xu
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs6" class="paper-title">
                            High Dimensional Sparse Linear Regression under Local Differential Privacy: Power and Limitations
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs6" class="panel-footer panel-paper-footer collapse">
                    In this paper, we study high dimensional sparse linear regression under the Local Differential Privacy (LDP) model, and give both negative and positive results. On the negative side, we show that polynomial dependency on the dimensionality $p$ of the space is unavoidable in the estimation error under the non-interactive local model, if the privacy of the whole dataset needs to be preserved. Similar limitations also exist for other types of error measurements and in the (sequential) interactive local. This indicates that differential privacy in high dimensional space is unlikely achievable for the problem. On the positive side, we show that the optimal rate of the error estimation can be made logarithmically depending on $p$ (i.e., $\log p$) under the local model, if only the privacy of the responses (labels) is to be preserved, where the upper bound is obtained by a new method called Differentially Private Iterative Hard Thresholding (DP-IHT), which is interesting in its own right.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Joshua Allen, Bolin Ding, Janardhan Kulkarni, Harsha Nori, Olga Ohrimenko and Sergey Yekhanin
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs7" class="paper-title">
                            An Algorithmic Framework For Differentially Private Data Analysis on Trusted Processors
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1807.00736" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs7" class="panel-footer panel-paper-footer collapse">
                    Differential privacy has emerged as the main definition for private big data analysis and machine learning. The {\em global} model of differential privacy, which assumes users trust the data collector, provides strong privacy guarantees and introduces small errors in the output. In contrast, applications of differential privacy in commercial systems by Apple, Google, and Microsoft, use the {\em local model}. Here users do not trust the data collector and hence randomize their data before sending it to the data collector. Unfortunately, local model is too strong for several important applications and hence is limited in its scope. In this work, we propose a framework based on trusted processors and a new definition of differential privacy called {\em Oblivious Differential Privacy}, which combines the best of both local and global models. The algorithms we design in this framework show interesting interplay of ideas from the streaming algorithms, oblivious algorithms, and differential privacy.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Antoine Boutet, Théo Jourdan and Carole Frindel
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs8" class="paper-title">
                            Toward privacy in IoT mobile devices for activity recognition
                        </a> &nbsp;&nbsp;
                        <a href="papers/13.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs8" class="panel-footer panel-paper-footer collapse">
                    Recent advances in wireless sensors for personal healthcare allow to recognise human real-time activities with mobile devices. While the analysis of those datas- tream can have many benefits from a health point of view, it can also lead to privacy threats by exposing highly sensitive information. In this paper, we propose a privacy-preserving framework for activity recognition. This framework relies on a machine learning technique to efficiently recognise the user activity pattern, useful for personal healthcare monitoring, while limiting the risk of re-identification of users from biometric patterns that characterizes each individual. To achieve that, we rely on a carefully features extraction scheme in both temporal and frequency domain and apply a generalisation-based approach on features leading to re-identify users. We extensively evaluate our framework with a reference dataset: results show an accurate activity recognition (87%) while limiting the re-identifation rate (33%). This represents a slightly decrease of utility (9%) against a large privacy improvement (53%) compared to state-of-the-art baselines.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Roshan Dathathri, Olli Saarikivi, Hao Chen, Kim Laine, Kristin Lauter, Saeed Maleki, Madanlal Musuvathi and Todd Mytkowicz
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs9" class="paper-title">
                            CHET: Compiler and Runtime for Homomorphic Evaluation of Tensor Programs
                        </a> &nbsp;&nbsp;
                        <a href="papers/14.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs9" class="panel-footer panel-paper-footer collapse">
                    Fully Homomorphic Encryption (FHE) provides the promise of performing arbitrary computations on encrypted data without requiring the decryption key. FHE can enable novel privacy-sensitive machine learning scenarios. However, programming FHE applications today is hard for non-FHE experts due to two challenges. First, achieving practical performance requires performing FHE-specific optimizations, including maximizing the vectorizing/batching capabilities of the underlying FHE scheme. Second, FHE schemes involve a careful choice of encryption parameters that tradeoff security for correctness, performance, and message bloat.
                    <br/>
                    This paper proposes CHET, a compiler and runtime for homomorphically evaluating tensor programs. Given a neural network specified as a high-level tensor circuit, CHET optimizes and compiles this circuit to an interface called the Homomorphic Instruction Set Architecture (HISA), which can then be targetted to different encryption libraries. CHET automatically chooses the encryption parameters and layouts that maximize performance for a given security and precision requirements. As a result, CHET generated code is faster than expert-optimized implementations.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Martin Bertran, Natalia Martinez, Afroditi Papadaki, Qiang Qiu, Miguel Rodrigues and Guillermo Sapiro
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs10" class="paper-title">
                            Learning Representations for Utility and Privacy: An Information-Theoretic Based Approach
                        </a> &nbsp;&nbsp;
                        <a href="papers/15.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs10" class="panel-footer panel-paper-footer collapse">
                    Protecting a secret while disclosing related information (utility) is a well known challenge in privacy; both users and service providers can and should collaborate to protect privacy, and this paper addresses this paradigm. Here we analyze the limits of information-theoretic privacy, and use these to design a data-driven privacy-preserving representation of the disclosed data X that is maximally informative about the utility variable U and minimally informative about the secret variable S. We describe important use-case scenarios where the utility providers are willing to collaborate, at least partially, with the sanitization process. In this setting, we limit the possible sanitization functions to space-preserving transformations, where the same algorithm can be used to infer the utility variable on both sanitized and unsanitized data. We illustrate this approach though two use cases; subject-within-subject, where we tackle the problem of having an identity detector (from facial images) that works only on a consenting subset of users; and emotion-and-gender, where we tackle the issue of hiding independent variables, as is the case of hiding gender while preserving emotion detection.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Florian Tramer and Dan Boneh
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs11" class="paper-title">
                            Slalom: Fast, Verifiable and Private Execution of Neural Networks in Trusted Hardware <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1806.03287" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs11" class="panel-footer panel-paper-footer collapse">
                    As Machine Learning (ML) gets applied to security-critical or sensitive domains, there is a growing need for integrity and privacy for outsourced ML computations. A pragmatic solution comes from Trusted Execution Environments (TEEs), which use hardware and software protections to isolate sensitive computations from the untrusted software stack. However, these isolation guarantees come at a price in performance, compared to untrusted alternatives. This paper initiates the study of high performance execution of Deep Neural Networks (DNNs) in TEEs by efficiently partitioning DNN computations between trusted and untrusted devices. Building upon an efficient outsourcing scheme for matrix multiplication, we propose Slalom, a framework that securely delegates execution of all linear layers in a DNN from a TEE (e.g., Intel SGX or Sanctum) to a faster, yet untrusted, co-located processor. We evaluate Slalom by executing DNNs in an Intel SGX enclave, which selectively delegates work to an untrusted GPU. For two canonical DNNs, VGG16 and MobileNet, we obtain 20× and 6× increases in throughput for verifiable inference, and 11× and 4× for verifiable and private inference.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Ashwin Machanavajjhala and Kamalika Chaudhuri
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs12" class="paper-title">
                            Capacity Bounded Differential Privacy
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs12" class="panel-footer panel-paper-footer collapse">
                    Differential privacy, a notion of algorithmic stability, is a gold standard for measuring the additional risk an algorithm's output poses to the privacy of any single record in the dataset. Differential privacy is defined as the distance between the output distribution of an algorithm on neighboring datasets that differ in one entry. In this work, we present a novel relaxation of differential privacy, called capacity bounded differential privacy, where the adversary that distinguishes output distributions is assumed to be capacity-bounded -- i.e. bounded not in computational power, but rather in terms of the function class from which their attack algorithm is drawn. We model adversaries in terms of restricted f-divergences between probability distributions, and study properties of the definition and algorithms that satisfy them.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Aurélien Bellet, Rachid Guerraoui and Hadrien Hendrikx
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs13" class="paper-title">
                            Who started this gossip? Differentially private rumor spreading
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs13" class="panel-footer panel-paper-footer collapse">
                    Gossip algorithms, also called rumor spreading or epidemic protocols, are widely used to disseminate information in massive peer-to-peer networks. These algorithms are often claimed to guarantee privacy because of the uncertainty they introduce on the node that started the dissemination. But is that claim really true? Can one indeed start a gossip and safely hide in the crowd?
                    <br/>
                    This paper is the first to study gossip protocols using the mathematical tools of differential privacy to determine the extent to which the source of a gossip can be traceable. Considering the case of a complete graph in which a subset of the nodes are curious sensors, we derive tight lower bounds on the differential privacy of the gossip source. The key to ensure differential privacy is to allow nodes to stop gossiping after some time, which essentially boils down to reducing the dissemination speed. Yet, we show that we can devise a gossip algorithm achieving both logarithmic spreading time and near-optimal privacy.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Koen Lennart van der Veen, Ruben Seggers, Peter Bloem and Giorgio Patrini
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs14" class="paper-title">
                            Three Tools for Practical Differential Privacy
                        </a> &nbsp;&nbsp;
                        <a href="papers/29.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs14" class="panel-footer panel-paper-footer collapse">
                    Differentially private learning on real-world data poses challenges for standard machine learning practice: privacy guarantees are difficult to interpret, hyperparameter tuning on private data reduces the privacy budget, and ad-hoc privacy attacks are often required to test model privacy. We introduce three tools to make differentially private machine learning more practical: (1) simple sanity checks which can be carried out in a centralized manner before training, (2) an adaptive clipping bound to reduce the effective number of tuneable privacy parameters, and (3) we show that large-batch training improves model performance.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Fabrice Benhamouda and Marc Joye
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs15" class="paper-title">
                            How to Profile Privacy-Conscious Users in Recommender Systems
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1812.00125" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs15" class="panel-footer panel-paper-footer collapse">
                    Matrix factorization is a popular method to build a recommender system. In such a system, existing users and items are associated to a low-dimension vector called a profile. The profiles of a user and of an item can be combined (via inner product) to predict the rating that the user would get on the item. One important issue of such a system is the so-called cold-start problem: how to allow a user to learn her profile, so that she can then get accurate recommendations?
                    <br/>
                    While a profile can be computed if the user is willing to rate well-chosen items and/or provide supplemental attributes or demographics (such as gender), revealing this additional information is known to allow the analyst of the recommender system to infer many more personal sensitive information. We design a protocol to allow privacy-conscious users to benefit from matrix-factorization-based recommender systems while preserving their privacy. More precisely, our protocol enables a user to learn her profile, and from that to predict ratings without the user revealing any personal information. The protocol is secure in the standard model against semi-honest adversaries.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Hao Chen, Ilaria Chillotti, Oxana Poburinnaya, Ilya Razenshteyn and M. Sadegh Riazi
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs16" class="paper-title">
                            Scaling Up Secure Nearest Neighbor Search
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs16" class="panel-footer panel-paper-footer collapse">
                    We present a new secure protocol for approximate nearest neighbor search over the Euclidean distance tailored for massive datasets in the semi-honest model. At a high level, our protocol combines additively homomorphic encryption (for distance computation) and garbled circuits (for top-k selection). To achieve good performance, we utilize several algorithmic and implementational improvements. In particular, we show the existence of a linear-sized circuit for approximate top-k selection. As an example, for the SIFT dataset of image descriptors (1 000 000 data points, 128 dimensions), our algorithm when run on two standard Azure instances can retrieve IDs of the 10 nearest neighbors with accuracy 0.92 in less than 4 seconds. This improves by more than an order of magnitude when compared to the alternative approaches.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Vitaly Feldman, Ilya Mironov, Kunal Talwar and Abhradeep Thakurta
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs17" class="paper-title">
                            Privacy Amplification by Iteration <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1808.06651" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs17" class="panel-footer panel-paper-footer collapse">
                    Many commonly used learning algorithms work by iteratively updating an intermediate solution using one or a few data points in each iteration. Analysis of differential privacy for such algorithms often involves ensuring privacy of each step and then reasoning about the cumulative privacy cost of the algorithm. This is enabled by composition theorems for differential privacy that allow releasing of all the intermediate results. In this work, we demonstrate that for contractive iterations, not releasing the intermediate results strongly amplifies the privacy guarantees.
                    <br/>
                    We describe several applications of this new analysis technique to solving convex optimization problems via noisy stochastic gradient descent. For example, we demonstrate that a relatively small number of non-private data points from the same distribution can be used to close the gap between private and non-private convex optimization. In addition, we demonstrate that we can achieve guarantees similar to those obtainable using the privacy-amplification-by-sampling technique in several natural settings where that technique cannot be applied.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Antti Koskela and Antti Honkela
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs18" class="paper-title">
                            Learning rate adaptation for differentially private stochastic gradient descent
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1809.03832" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs18" class="panel-footer panel-paper-footer collapse">
                    Differentially private learning has recently emerged as the leading approach for privacy-preserving machine learning. Differential privacy can complicate learning procedures because each access to the data needs to be carefully designed and carries a privacy cost. For example, standard parameter tuning with a validation set cannot be easily applied. We propose a differentially private algorithm for the adaptation of the learning rate for differentially private stochastic gradient descent (SGD) that avoids the need for validation set use. The idea for the adaptiveness comes from the technique of extrapolation in numerical analysis: to get an estimate for the error against the gradient flow which underlies SGD, we compare the result obtained by one full step and two half-steps. We prove the privacy of the method using the moments accountant mechanism. Empirically we can show that our method is competitive with manually tuned commonly used optimisation methods.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Yu-Xiang Wang, Borja Balle and Shiva Kasiviswanathan
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs19" class="paper-title">
                            Subsampled Renyi Differential Privacy and Analytical Moments Accountant <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1808.00087" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs19" class="panel-footer panel-paper-footer collapse">
                    We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms. Specifically, we provide a tight upper bound on the Renyi Differential Privacy (RDP) parameters for algorithms that: (1) subsample the dataset, and then (2) applies a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by Abadi et al. [CCS'16] for the Gaussian mechanism, to any subsampled RDP mechanism.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert and Jonathan Passerat-Palmbach
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs20" class="paper-title">
                            A generic framework for privacy preserving deep learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.04017" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs20" class="panel-footer panel-paper-footer collapse">
                    We detail a new framework for privacy preserving deep learning and discuss its assets. The framework puts a premium on ownership and secure processing of data and introduces a valuable representation based on chains of commands and tensors. This abstraction allows one to implement complex privacy preserving constructs such as Federated Learning, Secure Multiparty Computation, and Differential Privacy while still exposing a familiar deep learning API to the end-user. We report early results on the Boston Housing and Pima Indian Diabetes datasets. While the privacy features apart from Differential Privacy do not impact the prediction accuracy, the current implementation of the framework introduces a significant overhead in performance, which will be addressed at a later stage of the development. We believe this work is an important milestone introducing the first reliable, general framework for privacy preserving deep learning.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Valerie Chen, Valerio Pastro and Mariana Raykova
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs21" class="paper-title">
                            Secure Computation for Machine Learning With SPDZ
                        </a> &nbsp;&nbsp;
                        <a href="papers/44.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs21" class="panel-footer panel-paper-footer collapse">
                    Secure Multi-Party Computation (MPC) is an area of cryptography that enables computation on sensitive data from multiple sources while maintaining privacy guarantees. However, theoretical MPC protocols often do not scale efficiently to real-world data. This project investigates the efficiency of the SPDZ framework}, which provides an implementation of an MPC protocol with malicious security, in the context of popular machine learning (ML) algorithms. In particular, we chose applications such as linear regression and logistic regression, which have been implemented and evaluated using semi-honest MPC techniques. We demonstrate that the SPDZ framework outperforms these previous implementations while providing stronger security.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Kareem Amin, Travis Dick, Alex Kulesza, Andres Medina and Sergei Vassilvitskii
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs22" class="paper-title">
                            Private Covariance Estimation via Iterative Eigenvector Sampling
                        </a> &nbsp;&nbsp;
                        <a href="papers/45.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs22" class="panel-footer panel-paper-footer collapse">
                    We give a new method for computing a differentially private covariance matrix, suitable for use in a variety of regression settings, from a collection of user data. In contrast to previous methods, our approach has zero failure probability, and can be employed to compute the full matrix (not just the top k eigenvectors). As part of our analysis, we derive a precise expression for allocating the privacy budget epsilon among different components to minimize overall error. We then show empirically that our method outperforms previous state-of-the-art approaches, yielding lower error and better learning performance.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Alexandr Andoni, Tal Malkin and Negev Shekel Nosatzki
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs23" class="paper-title">
                            Secure Two Party Distribution Testing <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.04065" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs23" class="panel-footer panel-paper-footer collapse">
                    We study the problem of discrete distribution testing in the two-party setting. For example, in the standard closeness testing problem, Alice and Bob each have t samples from, respectively, distributions a and b over [n], and they need to test whether a=b or a,b are ε-far (in the ℓ1 distance) for some fixed ε>0. This is in contrast to the well-studied one-party case, where the tester has unrestricted access to samples of both distributions, for which optimal bounds are known for a number of variations. Despite being a natural constraint in applications, the two-party setting has evaded attention so far.
                    <br/>
                    We address two fundamental aspects of the two-party setting: 1) what is the communication complexity, and 2) can it be accomplished securely, without Alice and Bob learning extra information about each other's input. Besides closeness testing, we also study the independence testing problem, where Alice and Bob have t samples from distributions a and b respectively, which may be correlated; the question is whether a,b are independent of ε-far from being independent.
                    <br/><br/>
                    Our contribution is three-fold:
                    <br/>
                    ∙ Communication: we show how to gain communication efficiency as we have more samples, beyond the information-theoretic bound on t. Furthermore, the gain is polynomially better than what one may obtain by adapting one-party algorithms.
                    <br/>
                    For the closeness testing, our protocol has communication s=Θ_ε(n^2/t^2) as long as t is at least the information-theoretic minimum number of samples. For the independence testing over domain [n]×[m], where n≥m, we obtain s=O_ε(n^2m/t^2+nm/t+m^1/2).
                    <br/>
                    ∙ Security: we define the concept of secure distribution testing and argue that it must leak at least some minimal information when the promise is not satisfied. We then provide secure versions of the above protocols with an overhead that is only polynomial in the security parameter.
                    <br/>
                    ∙ Lower bounds: we prove tightness of our trade-off for the closeness testing, as well as that the independence testing requires tight Ω(m^1/2) communication for unbounded number of samples. These lower bounds are of independent interest as, to the best of our knowledge, these are the first 2-party communication lower bounds for testing problems, where the inputs represent a set of i.i.d. samples.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Phillipp Schoppmann, Adria Gascon, Mariana Raykova and Benny Pinkas
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs24" class="paper-title">
                            Make Some ROOM for the Zeros: Data Sparsity in Secure Distributed Machine Learning
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs24" class="panel-footer panel-paper-footer collapse">
                    Exploiting data sparsity is crucial for the scalability of many data analysis tasks. However, existing secure computation approaches for machine learning do not exploit sparsity, or do so in an implicit way in the context of custom protocols. In this work we mirror the development of the field of scientific computing and propose sparse data structures together with secure computation algorithms for common tasks that leverage sparsity for efficiency. We present several instantiations with different trade-offs for a Read-Only Oblivious Map (ROOM) primitive used to access elements in these structures. We build protocols for sparse matrix multiplication that use the ROOM functionality, which can be composed to implement secure variants of essential data analysis tasks such as similarity computation and gradient descent.
                    <br/>
                    We use our constructions for basic operations on sparse data to build secure protocols for non-parametric ($k$-nearest neighbors and naive Bayes classification) and parametric (linear and logistic regression) models that enable secure analysis on high-dimensional datasets. The experimental evaluation of our protocol implementations demonstrates manyfold improvement in the efficiency over state of the art techniques across all applications.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Judy Hoffman, Mehryar Mohri and Ningshan Zhang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs25" class="paper-title">
                            Algorithms and Theory for Multiple-Source Adaptation
                        </a> &nbsp;&nbsp;
                        <a href="papers/52.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs25" class="panel-footer panel-paper-footer collapse">
                    This work considers the multiple-source adaptation problem where the learner only has access to predictors and density estimations trained on source domains, but he has no access to all source data simultaneously. The goal is to combine source predictors to derive an accurate predictor for any unknown mixture target domain. We present the distribution-weighted combination solutions with strong theoretical guarantees for the general stochastic scenario under cross-entropy loss and other similar losses. Moreover, we give new algorithms for determining the solution for the cross-entropy loss and other losses. We report the results on a real-world dataset to show that our algorithm outperforms competing approaches by producing a single robust model that performs well on any target mixture distribution.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Kareem Amin, Alex Kulesza, Andres Munoz Medina and Sergei Vassilvitskii
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs26" class="paper-title">
                            Bias Variance Trade-off in Differential Privacy
                        </a> &nbsp;&nbsp;
                        <a href="papers/53.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs26" class="panel-footer panel-paper-footer collapse">
                    Differentially private learning algorithms protect individual participants in the training dataset by guaranteeing that their presence does not significantly change the resulting model. In order to make this promise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to protect them. While most existing analyses assume that the maximum contribution is known and fixed in advance we argue that in practice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end up adding excessive noise to protect a few outliers. On the other hand, limiting users to small contributions keeps noise levels low at the cost of potentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this tradeoff for an empirical risk minimization setting, showing that in general there is a “sweet spot” that depends on measurable properties of the dataset.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Sesh Kumar and Marc Deisenroth
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs27" class="paper-title">
                            Differentially Private Empirical Risk Minimization with Sparsity-Inducing Norms
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs27" class="panel-footer panel-paper-footer collapse">
                    Differential privacy is concerned about the prediction quality while measuring the privacy impact on individuals whose information is contained in the data.We consider differentially private risk minimization problems with regularizers that induce structured sparsity. These regularizers are known to be convex but are non-differentiable. We analyze the standard differentially private algorithms, such as output perturbation. Output perturbation is a differentially private algorithm that is known to perform well for minimizing risks that are strongly convex. Previous works have derived dimensionality independent excess risk bounds for these cases. In this paper, we assume a particular class of convex but non-smooth regularizers that induce structured sparsity and loss functions for generalized linear models. We derive excess risk bound for output perturbation that depends on the structure of the constraint set.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Morten Dahl, Jason Mancuso, Yann Dupis, Ben DeCoste, Morgan Giraud, Ian Livingstone, Justin Patriquin and Gavin Uhma
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs28" class="paper-title">
                            Private Machine Learning in TensorFlow using Secure Computation <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1810.08130" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs28" class="panel-footer panel-paper-footer collapse">
                    We present a framework for experimenting with secure multi-party computation directly in TensorFlow. By doing so we benefit from several properties valuable to both researchers and practitioners, including tight integration with ordinary machine learning processes, existing optimizations for distributed computation in TensorFlow, high-level abstractions for expressing complex algorithms and protocols, and an expanded set of familiar tooling. We give an open source implementation of a state-of-the-art protocol and report on concrete benchmarks using typical models from private machine learning.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Nicolas Loizou, Peter Richtarik, Filip Hanzely, Jakub Konecny and Dmitry Grishchenko
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs29" class="paper-title">
                            A Privacy Preserving Randomized Gossip Algorithm via Controlled Noise Insertion
                        </a> &nbsp;&nbsp;
                        <a href="papers/57.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs29" class="panel-footer panel-paper-footer collapse">
                    In this work we present a randomized gossip algorithm for solving the average consensus problem while at the same time protecting the information about the initial private values stored at the nodes. We give iteration complexity bounds for the method and perform extensive numerical experiments.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Yatharth Dubey and Aleksandra Korolova
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs30" class="paper-title">
                            The Power of The Hybrid Model for Mean Estimation <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.12040" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs30" class="panel-footer panel-paper-footer collapse">
                    In this work we explore the power of the hybrid model of differential privacy (DP) proposed in~\cite{blender}, where some users desire the guarantees of the local model of DP and others are content with receiving the trusted curator model guarantees. In particular, we study the accuracy of mean estimation algorithms for arbitrary distributions in bounded support. We show that a hybrid mechanism which combines the sample mean estimates obtained from the two groups in an optimally weighted convex combination performs a constant factor better for a wide range of sample sizes than natural benchmarks. We analyze how this improvement factor is parameterized by the problem setting and how it varies with sample size.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Ulfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar and Abhradeep Thakurta
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs31" class="paper-title">
                            Amplification by Shuffling: From Local to Central Differential Privacy via Anonymity <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.12469" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs31" class="panel-footer panel-paper-footer collapse">
                    Sensitive statistics are often collected across sets of users, with repeated collection of reports done over time. For example, trends in users' private preferences or software usage may be monitored via such reports. We study the collection of such statistics in the local differential privacy (LDP) model, when each user's value may change only a small number of times. We describe an algorithm whose privacy cost is polylogarithmic in the number of times the statistic is collected.
                    <br/>
                    More fundamentally---by building on anonymity of the users' reports---we also demonstrate how the privacy cost of our LDP algorithm can actually be much lower when viewed in the central model of differential privacy. We show, via a new and general technique, that any permutation-invariant algorithm satisfying $\varepsilon$-local differential privacy will satisfy $(O(\varepsilon \sqrt{\log \frac 1 \delta} / \sqrt{n}), \delta)$-central differential privacy. In the process, we clarify how the high noise and $\sqrt{n}$ overhead of LDP protocols is a consequence of them being significantly more private in the central model. As a final, practical corollary, our results also imply that industrial deployments of LDP mechanism may have much lower privacy cost than their advertised $\varepsilon$ would indicate---at least if reports are anonymized.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Vasyl Pihur, Aleksandra Korolova, Frederick Liu, Subhash Sankuratripati, Moti Yung, Dachuan Huang and Ruogu Zeng
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs32" class="paper-title">
                            Differentially Private "Draw and Discard" Machine Learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1807.04369" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs32" class="panel-footer panel-paper-footer collapse">
                    In this work, we propose a novel framework for privacy-preserving client-distributed machine learning. It is motivated by the desire to achieve differential privacy guarantees in the \emph{local} model of privacy in a way that satisfies all systems constraints using \emph{asynchronous} client-server communication and provides attractive model learning properties. We call it ``Draw and Discard'' because it relies on random sampling of models for load distribution (scalability), which also provides additional server-side privacy protections and improved model quality through averaging. We present the mechanics of client and server components of ``Draw and Discard" and demonstrate how the framework can be applied to learning Generalized Linear models. We then analyze the privacy guarantees provided by our approach against several types of adversaries and showcase experimental results that provide evidence for the framework's viability in practical deployments. We believe our framework is the first deployed distributed machine learning approach that operates in the local privacy model.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Brendan McMahan and Galen Andrew
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs33" class="paper-title">
                            A General Approach to Adding Differential Privacy to Iterative Training Procedures
                        </a> &nbsp;&nbsp;
                        <a href="papers/62.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs33" class="panel-footer panel-paper-footer collapse">
                    In this work we address the practical challenges of training machine learning models on privacy-sensitive datasets by introducing a modular approach that minimizes changes to training algorithms, provides a variety of configuration strategies for the privacy mechanism, and then isolates and simplifies the critical logic that computes the final privacy guarantees. A key challenge is that training algorithms often require estimating many different quantities (vectors) from the same set of examples --- for example, gradients of different layers in a deep learning architecture, as well as metrics and batch normalization parameters. Each of these may have different properties like dimensionality, approximate norm, and tolerance to noise. By extending previous work on the Moments Accountant for the subsampled Gaussian mechanism, we can provide privacy for such heterogeneous sets of vectors, while also structuring the approach to minimize software engineering challenges.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Alexandra Schofield, Aaron Schein, Zhiwei Steven Wu and Hanna Wallach
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs34" class="paper-title">
                            A Variational Inference Approach for Locally Private Inference of Poisson Factorization Models
                        </a> &nbsp;&nbsp;
                        <a href="papers/63.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs34" class="panel-footer panel-paper-footer collapse">
                    Recent work that introduces local privacy to Bayesian Poisson factorization model inference, but its proposed MCMC algorithm for inference suffers from significant performance issues. We derive a coordinate ascent variational inference algorithm to infer factorization models efficiently from private data. Our model relies on several new properties we prove about Bessel distributions. We demonstrate that this produces a factor of 20 speedup in a synthetic experiment for model inference.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Siddharth Garg, Zahra Ghodsi, Carmit Hazay, Yuval Ishai, Antonio Mercedone and Muthuramakrishnan Venkitasubramaniam
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs35" class="paper-title">
                            Outsourcing Private Machine Learning via Lightweight Secure Arithmetic Computation
                        </a> &nbsp;&nbsp;
                        <!-- <a href="papers/paper.pdf" class="link-paper">[PDF]</a> -->
                    </div>
                    <div id="abs35" class="panel-footer panel-paper-footer collapse">
                    In several settings of practical interest, two parties seek to collaboratively perform inference on their private data using a public machine learning model. For instance, several hospitals might wish to share patient medical records for enhanced diagnostics and disease prediction, but may not be able to share data in the clear because of privacy concerns. In this work, we propose an actively secure protocol for outsourcing secure and private machine learning computations. Recent works on the problem have mainly focused on passively secure protocols, whose security holds against passive (``semi-honest'') parties but may completely break down in the presence of active (``malicious'') parties who can deviate from the protocol. Secure NN based classification algorithms can be an seen as an instantiation of an arithmetic computation over integers. We showcase the efficiency of our protocol by applying it to real-world instances of arithmeticized neural network computations, including a network trained to perform collaborative disease prediction.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Frederik Harder, Jonas Köhler, Max Welling and Mijung Park
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs36" class="paper-title">
                            DP-MAC: The Differentially Private Method of Auxiliary Coordinates for Deep Learning <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="papers/67.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs36" class="panel-footer panel-paper-footer collapse">
                    Developing a differentially private deep learning algorithm is challenging, due to the difficulty in analyzing the sensitivity of objective functions that are typically used to train deep neural networks. Many existing methods resort to the stochastic gradient descent algorithm and apply a pre-defined sensitivity to the gradients for privatizing weights. However, their slow convergence typically yields a high cumulative privacy loss. Here, we take a different route by employing the method of auxiliary coordinates, which allows us to independently update the weights per layer by optimizing a per-layer objective function. This objective function can be well approximated by a low-order Taylor's expansion, in which sensitivity analysis becomes tractable. We perturb the coefficients of the expansion for privacy, which we optimize using more advanced optimization routines than SGD for faster convergence. We empirically show that our algorithm provides a decent trained model quality under a modest privacy budget.
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            Da Yu, Huishuai Zhang and Wei Chen
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs37" class="paper-title">
                            Improving the Gradient Perturbation Approach for Differentially Private Optimization
                        </a> &nbsp;&nbsp;
                        <a href="papers/70.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs37" class="panel-footer panel-paper-footer collapse">
                    We investigate the gradient perturbation approach for differentially private (DP) optimization and improve its performance by utilizing the optimization properties of gradient descent. One property is that historical gradient are often used as a momentum to facilitate the convergence. However, momentum gradient descent with usual coefficient does not perform well in the DP case. Instead, we specifically choose a coefficient based on our analysis. Another property is that the gradient tends to zero as optimization algorithm converges. Inspired by this, we intentionally decrease the gradient norm clip bound, which corresponds to decrease the sensitivity in the differential privacy computation. This allows us to run more training steps while keeping the same privacy guarantee. Our approaches are applicable for both convex and non-convex optimizations with differential privacy constraint. We demonstrate that our approaches can significantly improve the training/test performance under a privacy budget via various experiments.
                    </div>
                </div>

        </div>
        </div>
    </section>

    <!-- Call for travel grants -->
        <section id="grants" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Travel Grants</h2>
                    <p>
                    Thanks to our generous sponsors, we are able to provide a limited number of travel grants of up to $800 to help partially cover the expenses of authors of accepted papers who have not received other travel support from NeurIPS this year.
                    To apply, please send an email to <a href="mailto:ppml18@easychair.org?Subject=PPML18%20Travel%20Grant%20Application">ppml18@easychair.org</a> with the subject “PPML18 Travel Grant Application” including your resume and a half-page statement of purpose mentioning the title and the authors of your accepted paper and a summary of anticipated travel expenses. If you are an undergraduate or graduate student, we ask for a half-page recommendation letter supporting your application to be sent to us by the deadline. The deadline for applications is <b>November 11, 2018 (11:59pm AoE)</b>. The notifications will be sent by <b>November 16</b>. Please feel free to send us an email if you have any questions.
                </div>
            </div>
        </section>

    <!-- Organizers Section -->
    <section id="organizers" class="content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Organization</h2>
                <br />
                <h3>Workshop organizers</h3>
                <ul class="list-group">
                    <li class="list-group-item organizer">Aurélien Bellet (Inria)</li>
                    <li class="list-group-item organizer">Adrià Gascón (Alan Turing Institute & Warwick)</li>
                    <li class="list-group-item organizer">Niki Kilbertus (MPI for Intelligent Systems & Cambridge)</li>
                    <li class="list-group-item organizer">Olya Ohrimenko (Microsoft Research)</li>
                    <li class="list-group-item organizer">Mariana Raykova (Yale)</li>
                    <li class="list-group-item organizer">Adrian Weller (Alan Turing Institute & Cambridge)</li>
                </ul>
                <br />
                <h3>Program Committee</h3>
                <ul class="list-group">
                    <li class="list-group-item organizer">Pauline Anthonysamy (Google)</li>
                    <li class="list-group-item organizer">Borja de Balle Pigem (Amazon)</li>
                    <li class="list-group-item organizer">James Bell (University of Cambridge)</li>
                    <li class="list-group-item organizer">Battista Biggio (University of Cagliari)</li>
                    <li class="list-group-item organizer">Keith Bonawitz (Google)</li>
                    <li class="list-group-item organizer">Graham Cormode (University of Warwick)</li>
                    <li class="list-group-item organizer">Morten Dahl (Dropout Labs)</li>
                    <li class="list-group-item organizer">Emiliano de Cristofaro (University College London)</li>
                    <li class="list-group-item organizer">Christos Dimitrakakis</li>
                    <li class="list-group-item organizer">David Evans (University of Virginia)</li>
                    <li class="list-group-item organizer">Joseph Geumlek (UCSD)</li>
                    <li class="list-group-item organizer">Irene Giacomelli (Wisconsin University)</li>
                    <li class="list-group-item organizer">Stephen Hardy (Data61)</li>
                    <li class="list-group-item organizer">Stratis Ioannidis (Northeastern University)</li>
                    <li class="list-group-item organizer">Peter Kairouz (Stanford)</li>
                    <li class="list-group-item organizer">Nadin Kokciyan (King's College London)</li>
                    <li class="list-group-item organizer">Aleksandra Korolova (USC)</li>
                    <li class="list-group-item organizer">Kim Laine (Microsoft Research)</li>
                    <li class="list-group-item organizer">Ashwin Machanavajjhala (Duke University)</li>
                    <li class="list-group-item organizer">Payman Mohassel (Visa Research)</li>
                    <li class="list-group-item organizer">Catuscia Palamidessi (École Polytechnique & INRIA)</li>
                    <li class="list-group-item organizer">Mijung Park (Max Planck Institute for Intelligent Systems)</li>
                    <li class="list-group-item organizer">Giorgio Patrini (University of Amsterdam)</li>
                    <li class="list-group-item organizer">Benjamin Rubinstein (University of Melbourne)</li>
                    <li class="list-group-item organizer">Anand Sarwate (Rutgers University)</li>
                    <li class="list-group-item organizer">Phillipp Schoppmann (HU Berlin)</li>
                    <li class="list-group-item organizer">Nigel Smart (KU Leuven)</li>
                    <li class="list-group-item organizer">Carmela Troncoso (EPFL)</li>
                    <li class="list-group-item organizer">Yu-Xiang Wang (UCSB)</li>
                    <li class="list-group-item organizer">Pinar Yolum (Utrecht University)</li>
                    <li class="list-group-item organizer">Samee Zahur (Google)</li>
                </ul>
                <br />
                <h3>Sponsors</h3>
                <br />
                    <img style="margin:50px;"height="80" src="img/ati-white.png">
                    <img style="margin:50px;"height="80" src="img/amazon.png">
                    <img style="margin:50px;"height="80" src="img/google.png">
                    <img style="margin:50px;"height="80" src="img/microsoft.png">
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>Contact us: <a href="mailto:ppml18@easychair.org">ppml18@easychair.org</a></p>
            <br/>
        </div>

    </footer>

    <!-- jQuery -->
    <script src="js/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/script.js"></script>

</body>

</html>
